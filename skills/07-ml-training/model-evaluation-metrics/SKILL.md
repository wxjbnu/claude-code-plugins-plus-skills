---
name: "model-evaluation-metrics"
description: |
  Model Evaluation Metrics - Auto-activating skill for ML Training.
  Triggers on: model evaluation metrics, model evaluation metrics
  Part of the ML Training skill category. Use when working with model evaluation metrics functionality. Trigger with phrases like "model evaluation metrics", "model metrics", "model".
allowed-tools: "Read, Write, Edit, Bash(python:*), Bash(pip:*)"
version: 1.0.0
license: MIT
author: "Jeremy Longshore <jeremy@intentsolutions.io>"
---

# Model Evaluation Metrics

## Purpose

This skill provides automated assistance for model evaluation metrics tasks within the ML Training domain.

## When to Use

This skill activates automatically when you:
- Mention "model evaluation metrics" in your request
- Ask about model evaluation metrics patterns or best practices
- Need help with machine learning training skills covering data preparation, model training, hyperparameter tuning, and experiment tracking.

## Capabilities

- Provides step-by-step guidance for model evaluation metrics
- Follows industry best practices and patterns
- Generates production-ready code and configurations
- Validates outputs against common standards

## Example Triggers

- "Help me with model evaluation metrics"
- "Set up model evaluation metrics"
- "How do I implement model evaluation metrics?"

## Related Skills

Part of the **ML Training** skill category.
Tags: ml, training, pytorch, tensorflow, sklearn
